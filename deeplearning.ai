## 卷积神经网络
1.1 计算机视觉主要处理分类、识别、生成特殊效果等任务。
分类：判断图片中的物体类别；
识别：标记出图片中物体的位置；
1.2 边缘检测示例
通过人为设定3x3卷积核的数值，识别图像像素边缘：
竖边缘检测核：
1 0 -1
1 0 -1
1 0 -1
横边缘检测核：
1 1 1
0 0 0
-1-1-1
Sobel算子：
1 0 -1
2 0 -2
1 0 -1
Scharr算子：
3   0  -3
10 0 -10
3   0  -3
1.3 算子能否自动生成最优
由于人为设定能检测水平和竖直方向的边缘，那斜线如何检测？
如果设定3x3为未知数，通过大量图片学习，能否自动学习到不同边缘的最优检测方法。深度学习就是基于这样一种基础机制来工作的。
1.4 Padding：为了不损失边缘像素，保持每个像素计算次数相等
1.5 Stride：跨度，NxN * FxF -> (N - F + 1)/S x (N - F + 1)/S
1.6 多通道（立体卷积核）：图像数据是多维数据，卷积核也应该是多维的；另外多个卷积核可以提取不同特征
1.7 Z = WxX + b； a = g(Z)
1.9 池化：提高算法鲁棒性，减少运算量
1.10 LeNet介绍：CONV1+POOL1 -> CONV2+POOL2（Flat 400） -> FC3（120） -> FC4（84）-> 10(0~9)
1.11. why use 卷积：参数共享 + 稀疏特征
参数共享：同一特征在图中不同位置反复出现，能有效减少参数和计算量
稀疏特征：局部区域特征基本不受区域外影响

## 深度卷积网络
2.1通过实例学习，可以了解网络发展情况，各种网络设计的原因，解决了哪些问题
2.2经典网络介绍：LeNet5 -> Alex网络 -> VGG -> ResNet -> inception
LeNet5：图像尺寸变小，卷积核变深
Alex：深层网络，CONV和POOL叠加，全链接，Relu激活
VGG：多层网络简单叠加，使用3x3卷积核
2.3ResNet：解决的网络变深，训练误差变大的问题，让网络可以足够深
参考：https://zhuanlan.zhihu.com/p/31502877 Resnet源代码解读。
1x1卷积核使用，残差块：（2x（CONV+Active）+CONV）+ ShortCut，跟一个CONV
2.4残差网络为什么有效：通过添加shortcut，至少可以保证网络加深，学习效果不会降低；通过更多数据学习，Shortcut能学习到更优的解，同时低层次的特征在更后面的网络中有一定程度的保留。
2.5重点介绍1x1网络：多种特征组合，增强非线性激活，减少运算量
2.6GoogLeNet：不同尺寸卷积核组合叠加使用，通过参数学习避免将卷积核作为超参数来设定。
由于要保持输出尺寸一致，所以需要Padding；使用1x1卷积核减少参数和运算量
2.9迁移学习：使用现成模型及参数和自己问题的数据，进行finetuning。可以通过将数据预先处理成bootleneck存入磁盘，后续直接用其训练
2.10数据扩充：
图片变换：镜像、裁剪、旋转，PCA色彩变化等
2.11人工设计特征工程和完全数据驱动：
二者结合，通常人工预处理数据 + 数据驱动效果较好。

## 目标检测
3.1目标定位：分类、分类+定位、多类别检测（分类和定位）
网络结构有差异，数据标签越来越复杂，从单一的one-hot到额外增加定位信息，到多个类别+定位信息
3.2Landmark检测：标签由特征点个数决定，比如人脸识别，在人脸内找出64对坐标，他们分别表示了人脸上的主要特征的位置。
3.3目标检测：识别出物体类别，并检测出其位置
3.4滑动窗口实现目标检测：通过不同尺度的滑动窗口逐次运行识别网络，效率非常低
3.5
